{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyCatruna/DSM/blob/main/Lab_05a_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DutiRbgn0lA"
      },
      "source": [
        "## Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpiyX24kNAfi"
      },
      "source": [
        "Semantic Segmentation refers to the task of assigning a specific class to each pixel in an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuutX7gEJiXE"
      },
      "source": [
        "<img src=\"https://i0.wp.com/cdn-images-1.medium.com/max/850/1*f6Uhb8MI4REGcYKkYM9OUQ.png?w=850&resize=850,662&ssl=1\" width=600>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9M9G-O2QUcm"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6FYvwz5oFsI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.segmentation import MeanIoU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsMP0y1xQi6P"
      },
      "source": [
        "We will use the [Pascal VOC Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html) for this lab.\n",
        "\n",
        "The dataset has 20 classes (21 if including background).\n",
        "\n",
        "We will keep only 3 classes (Background, Person, and Dog) to make training easier. We will discard the rest of the samples that do not include these classes.\n",
        "\n",
        "The following code performs this filtering. You can ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5uK_A8GPNah"
      },
      "outputs": [],
      "source": [
        "''' Code for filtering the dataset '''\n",
        "\n",
        "CLASS_MAPPING = {\n",
        "    0: 0,  # Background stays background\n",
        "    15: 1,  # Person class\n",
        "    12: 2,   # Dog class\n",
        "}\n",
        "num_classes = len(CLASS_MAPPING)\n",
        "\n",
        "# Remap target classes in segmentation masks\n",
        "def remap_mask(mask):\n",
        "    mask[torch.isin(mask, torch.tensor(list(CLASS_MAPPING.keys()))) == 0] = 0\n",
        "    for old_class, new_class in CLASS_MAPPING.items():\n",
        "        mask[mask == old_class] = new_class\n",
        "    return mask\n",
        "\n",
        "class FilteredVOCSegmentation(Dataset):\n",
        "    def __init__(self, root='./data', image_set='train', download=True, transform=None, target_transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = VOCSegmentation(\n",
        "            root=root, \n",
        "            image_set=image_set, \n",
        "            download=download, \n",
        "            transform=transform, \n",
        "            target_transform=target_transform\n",
        "        )\n",
        "\n",
        "        self.filtered_dataset = self.filter_dataset()\n",
        "        if image_set == 'train':\n",
        "            self.class_weights = self.calculate_class_weights()\n",
        "            print(f'Class weights: {self.class_weights}')\n",
        "\n",
        "    def calculate_class_weights(self):\n",
        "        class_counts = torch.zeros(len(CLASS_MAPPING))\n",
        "        for _, mask in self.filtered_dataset:\n",
        "            for class_idx in range(len(CLASS_MAPPING)):\n",
        "                class_counts[class_idx] += torch.sum(mask == class_idx)\n",
        "        total = torch.sum(class_counts)\n",
        "        class_weights = total / class_counts\n",
        "\n",
        "        return class_weights\n",
        "\n",
        "    def filter_dataset(self):\n",
        "        filtered_indices = []\n",
        "        for idx, (img, mask) in enumerate(self.dataset):\n",
        "          if torch.unique(mask).tolist() != [0]:\n",
        "            filtered_indices.append(idx)\n",
        "\n",
        "        filtered_dataset = torch.utils.data.Subset(self.dataset, filtered_indices)\n",
        "        return filtered_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.filtered_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.filtered_dataset[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo5c6jUmzcEV"
      },
      "outputs": [],
      "source": [
        "''' Code for obtaining the dataloaders '''\n",
        "\n",
        "# Training transforms - You can modify these to obtain better results but you need to make sure you also modify the masks\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Test transforms\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Mask transforms - We also apply image transforms to the labels\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "    transforms.PILToTensor(),\n",
        "    remap_mask\n",
        "])\n",
        "\n",
        "train_dataset = FilteredVOCSegmentation(\n",
        "    root='./data',\n",
        "    image_set='train',\n",
        "    download=True,\n",
        "    transform=train_transform,\n",
        "    target_transform=mask_transform\n",
        ")\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "test_dataset = FilteredVOCSegmentation(\n",
        "    root='./data',\n",
        "    image_set='val',\n",
        "    download=True,\n",
        "    transform=test_transform,\n",
        "    target_transform=mask_transform\n",
        ")\n",
        "\n",
        "testloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw3wCoaJ9DaO"
      },
      "outputs": [],
      "source": [
        "''' Code for visualizing images, ground truth masks, and segmentation predictions '''\n",
        "\n",
        "def visualize_images_and_masks(images, masks, predicted_masks=None, num_samples=5):\n",
        "  images = images.cpu()\n",
        "  masks = masks.cpu()\n",
        "\n",
        "  num_cols = 2\n",
        "  if predicted_masks is not None:\n",
        "    predicted_masks = predicted_masks.cpu()\n",
        "    num_cols = 3\n",
        "\n",
        "  # De-normalize image\n",
        "  mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "  std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "  images = images * std + mean\n",
        "  images = torch.clamp(images, 0, 1)\n",
        "\n",
        "  fig, axes = plt.subplots(num_samples, num_cols, figsize=(6, 3 * num_samples))\n",
        "  for i in range(num_samples):\n",
        "    if num_cols == 3:\n",
        "      ax_img, ax_mask, ax_pred_mask = axes[i]\n",
        "    else:\n",
        "      ax_img, ax_mask = axes[i]\n",
        "\n",
        "    # Plot the image\n",
        "    ax_img.imshow(images[i].permute(1, 2, 0))\n",
        "    ax_img.set_title(\"Image\")\n",
        "    ax_img.axis(\"off\")\n",
        "\n",
        "    # Plot the mask\n",
        "    ax_mask.imshow(masks[i].squeeze(0), cmap=\"Accent\", vmin=0, vmax=num_classes)\n",
        "    ax_mask.set_title(\"Mask\")\n",
        "    ax_mask.axis(\"off\")\n",
        "\n",
        "    # Plot the predicted masks\n",
        "    if predicted_masks is not None:\n",
        "      ax_pred_mask.imshow(predicted_masks[i].squeeze(0), cmap=\"Accent\", vmin=0, vmax=num_classes)\n",
        "      ax_pred_mask.set_title(\"Predicted Mask\")\n",
        "      ax_pred_mask.axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk2Lm-tLJ3jJ"
      },
      "outputs": [],
      "source": [
        "# Visualize 5 images and masks from the train_loader\n",
        "images, masks = next(iter(trainloader))\n",
        "visualize_images_and_masks(images, masks, num_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNq16rauoTlg"
      },
      "source": [
        "#### U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE4KSiC-p7yR"
      },
      "source": [
        "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlUMB-buSJ0M"
      },
      "source": [
        "We will build a segmentation model from scratch.\n",
        "\n",
        "We will try to reproduce U-Net with one small difference - At each stage we will extract half the feature maps as in the original model (32 instead of 64, 64 instead of 128 etc.) so as to have less parameters.\n",
        "\n",
        "Below is the code for the core modules of the U-Net."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB4HhwUQniJf"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  ''' Single Convolution followed by BatchNorm and activation (ReLU) - blue arrow in image '''\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "class DoubleConvBlock(nn.Module):\n",
        "  ''' Two consecutive convolution blocks - double blue arrow in image '''\n",
        "  def __init__(self, in_channels, out_channels, intermediary_channels=None):\n",
        "    super().__init__()\n",
        "    if not intermediary_channels:\n",
        "        intermediary_channels = out_channels\n",
        "\n",
        "    self.double_conv = nn.Sequential(\n",
        "        ConvBlock(in_channels, intermediary_channels),\n",
        "        ConvBlock(intermediary_channels, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv(x)\n",
        "\n",
        "class Downscale(nn.Module):\n",
        "  ''' Apply max pooling to reduce spatial size then double conv block - red arrow followed by double blue arrow in image '''\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv = DoubleConvBlock(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.maxpool(x)\n",
        "    return self.conv(x)\n",
        "\n",
        "class Upscale(nn.Module):\n",
        "  '''Upscale with Transpose convolution then apply conv block - green arrow followed by double blue arrow in image '''\n",
        "\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "    self.conv = DoubleConvBlock(in_channels, out_channels)\n",
        "\n",
        "  ''' Input is: 1. output of previous module and 2. output of corresponding Downsample block - gray arrow in the image '''\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "    x = torch.cat([x2, x1], dim=1)\n",
        "\n",
        "    return self.conv(x)\n",
        "\n",
        "class OutputModule(nn.Module):\n",
        "  '''1x1 Convolution to obtain the prediction - out_channels is equal to number of classes - light blue arrow in image '''\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amzHAWdCTH4F"
      },
      "source": [
        "The UNet has 4 Downscale modules and 4 Upscale modules.\n",
        "\n",
        "The Upscale operations also take as input the features obtained by the corresponding Downscale modules (the gray arrows in the image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9xKMbMzv6Cv"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, n_channels, n_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    # Initial convolutions\n",
        "    self.initial = DoubleConvBlock(n_channels, 32)\n",
        "\n",
        "    # Downsampling part\n",
        "    self.down1 = Downscale(32, 64)\n",
        "    self.down2 = Downscale(64, 128)\n",
        "    self.down3 = Downscale(128, 256)\n",
        "    self.down4 = Downscale(256, 512)\n",
        "\n",
        "    # Upsampling part\n",
        "    self.up1 = Upscale(512, 256)\n",
        "    self.up2 = Upscale(256, 128)\n",
        "    self.up3 = Upscale(128, 64)\n",
        "    self.up4 = Upscale(64, 32)\n",
        "    self.out = OutputModule(32, n_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Initial convolutions\n",
        "    x1 = self.initial(x)\n",
        "\n",
        "    # Downsampling part - we need to keep the outputs for the upsampling part\n",
        "    x2 = self.down1(x1)\n",
        "    x3 = self.down2(x2)\n",
        "    x4 = self.down3(x3)\n",
        "    x5 = self.down4(x4)\n",
        "\n",
        "    # Upscaling part\n",
        "    x = self.up1(x5, x4)\n",
        "    x = self.up2(x, x3)\n",
        "    x = self.up3(x, x2)\n",
        "    x = self.up4(x, x1)\n",
        "\n",
        "    return self.out(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypB-rP0Gw6De"
      },
      "outputs": [],
      "source": [
        "unet = UNet(n_channels=3, n_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVlTVK-hIEKb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX924djBIE6E"
      },
      "outputs": [],
      "source": [
        "unet.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNAv4T3QyyIw"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"The model has {count_parameters(unet):,} trainable parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxxtf4VLU1sA"
      },
      "source": [
        "The code for training is very similar to that for classification. The only difference is that now we apply the cross-entropy loss at the pixel level.\n",
        "\n",
        "For validation we utilize the Mean Intersection over Union (mIoU) - you can read more about it [here](https://www.jeremyjordan.me/evaluating-image-segmentation-models/)\n",
        "\n",
        "Intersection over Union is simply computed as:\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*2w493Z_V6-sE_3aYa48a9w.png\">\n",
        "\n",
        "\n",
        "\n",
        "The metric can take values in the range 0-1:\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*kK0G-BmCqigHrc1rXs7tYQ.jpeg\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC2kj5De56f-"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, device, optimizer, criterion, epoch):\n",
        "    # We set the model to be in training mode\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    # We iterate through all batches - 1 step is 1 batch of batch_size images\n",
        "    for step, (images, labels) in bar:\n",
        "        # We take the images and their labels and push them on GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).squeeze(1)\n",
        "        labels = labels.long()\n",
        "        batch_size = images.shape[0]\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Obtain predictions\n",
        "        pred = model(images)\n",
        "        # Compute loss for this batch\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        # Compute gradients for each weight (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights based on gradients (gradient descent)\n",
        "        optimizer.step()\n",
        "\n",
        "        # We keep track of the average training loss\n",
        "        total_train_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "def valid_epoch(model, dataloader, device, criterion, epoch):\n",
        "    # We set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    iou_metric = MeanIoU(num_classes=num_classes, input_format='index').to(device)\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).squeeze(1)\n",
        "        labels = labels.long()\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "\n",
        "        # Compute IoU for each class\n",
        "        iou_metric.update(predicted, labels)\n",
        "\n",
        "        total_val_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss)\n",
        "\n",
        "    mean_iou = iou_metric.compute()\n",
        "    print(f\"Mean IoU: {mean_iou}\")\n",
        "    return mean_iou, epoch_loss\n",
        "\n",
        "def run_training(model, num_epochs, learning_rate):\n",
        "    # Define criterion\n",
        "    criterion = nn.CrossEntropyLoss(weight=train_dataset.class_weights).to(device)\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Check if we are using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    # For keeping track of the best validation accuracy\n",
        "    top_miou = 0.0\n",
        "\n",
        "    # We train the emodel for a number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, criterion, epoch)\n",
        "\n",
        "        # For validation we do not keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            mean_iou, val_loss = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if mean_iou > top_miou:\n",
        "                print(f\"Mean IoU Improved ({top_miou} ---> {mean_iou})\")\n",
        "                top_miou = mean_iou\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RXNCpF3I5_b"
      },
      "outputs": [],
      "source": [
        "run_training(unet, num_epochs=25, learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52bRrg2bQQGR"
      },
      "outputs": [],
      "source": [
        "# Visualize 5 predictions from training set - re-run this cell to look at other predictions\n",
        "images, masks = next(iter(trainloader))\n",
        "images = images.to(device)\n",
        "masks = masks.to(device)\n",
        "predicted_masks = unet(images)\n",
        "predicted_masks = torch.argmax(predicted_masks, dim=1)\n",
        "\n",
        "visualize_images_and_masks(images, masks, predicted_masks, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XHmqj7sQQGS"
      },
      "outputs": [],
      "source": [
        "# Visualize 15 predictions from testing set\n",
        "images, masks = next(iter(testloader))\n",
        "images = images.to(device)\n",
        "masks = masks.to(device)\n",
        "predicted_masks = unet(images)\n",
        "predicted_masks = torch.argmax(predicted_masks, dim=1)\n",
        "\n",
        "visualize_images_and_masks(images, masks, predicted_masks, num_samples=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmO9l28bXk1d"
      },
      "source": [
        "**Exercise** - Fine-tune a pre-trained model for image segmentation on this dataset\n",
        "\n",
        "Our model could not learn very much as the number of samples is low and we did not use augmentations. Use a pre-traind model to obtain better results.\n",
        "\n",
        "You can use one from [here](https://github.com/qubvel-org/segmentation_models.pytorch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8Eq5cKGdIbD"
      },
      "outputs": [],
      "source": [
        "!pip install -q segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClPMa0wAXiS8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load a pre-trained DeepLabV3+ model with ResNet backbone\n",
        "model = smp.DeepLabV3Plus(\n",
        "    encoder_name=\"resnet50\",      # Pretrained ResNet50 backbone\n",
        "    encoder_weights=\"imagenet\",   # Use ImageNet pre-trained weights\n",
        "    classes=num_classes,          # Number of output classes in segmentation\n",
        "    activation=None               # No activation, since we use CrossEntropyLoss\n",
        ").to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss(weight=train_dataset.class_weights.to(device))\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "run_training(model, num_epochs=20, learning_rate=learning_rate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
