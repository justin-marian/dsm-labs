{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyCatruna/DSM/blob/main/Lab_01_Neural_Networks_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d-6DC2Dof61"
      },
      "source": [
        "## Introduction to PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjBzyO7wweAi"
      },
      "source": [
        "Additional Resources for learning about PyTorch and working with neural networks:\n",
        "- [PyTorch official tutorials](https://pytorch.org/tutorials/)\n",
        "- [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
        "- [Neural Networks: Zero to Hero by Andrej Karpathy](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n",
        "- [Learn PyTorch for deep learning in a day by Daniel Bourke](https://www.youtube.com/watch?v=Z_ikDlimN6A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAdMzV81omQm"
      },
      "source": [
        "The purpose of this lab is to offer a short introduction to the PyTorch library and to help you construct and train a neural network.\n",
        "\n",
        "You should be familiar with numpy basics. Here is a short [tutorial](https://numpy.org/devdocs/user/quickstart.html) on numpy operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCmah8dG6yUQ"
      },
      "source": [
        "We import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuWk21oSoVIJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fukChWP6p83z"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "PyTorch Tensors are similar to Numpy arrays, but have support for GPU acceleration and gradient computation.\n",
        "\n",
        "A tensor is a generalization of data structures that you are familiar with.\n",
        "\n",
        "For example a vector is a 1D tensor, and a matrix is a 2D tensor. Most operations with torch tensors are similar to those of Numpy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFodaKIXrZQr"
      },
      "source": [
        "#### Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQEYDBZaE7xe"
      },
      "source": [
        "Creating an empty tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiLd2AHsrc5Z"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([])\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4wtOdUXFAF7"
      },
      "source": [
        "Creating a tensor with specific size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfWTqKx0r5Iv"
      },
      "outputs": [],
      "source": [
        "b = torch.empty((3, 4))  # Creates a tensor of size 3x4 filled with uninitialized values\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6YLaO3SFGYe"
      },
      "source": [
        "Creating a tensor from a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4E9P0f3r-at"
      },
      "outputs": [],
      "source": [
        "c = torch.tensor([[1, 2], [3, 4]])\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt3PDMosFKNG"
      },
      "source": [
        "Converting a NumPy array to a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF7NCseAsCG6"
      },
      "outputs": [],
      "source": [
        "array_data = np.array([1, 2, 3, 4, 5])\n",
        "d = torch.from_numpy(array_data)\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qDXl4DzFNzn"
      },
      "source": [
        "Creating a tensor with random values between 0 and 1 with specific shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il9OwnevsXMG"
      },
      "outputs": [],
      "source": [
        "e = torch.rand(2, 2)\n",
        "print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_5DNlL4svEu"
      },
      "source": [
        "#### Shape of tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIDrC8xYFVEa"
      },
      "source": [
        "The dimensions of a tensor can be checked with `.shape` attribute or `.size()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRVu_8GKstA9"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(5,3,2)\n",
        "\n",
        "shape = x.shape\n",
        "print(\"Shape:\", x.shape)\n",
        "\n",
        "size = x.size()\n",
        "print(\"Size:\", size)\n",
        "\n",
        "dim1, dim2, dim3 = x.size()\n",
        "print(\"Size:\", dim1, dim2, dim3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYiujLGYtM0l"
      },
      "source": [
        "#### Operations with tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-02igpQFfyE"
      },
      "source": [
        "Tensors support most mathematical operations, similar to numpy arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyaVRCTnFte3"
      },
      "source": [
        "Addition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98dWnk_JtPve"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([[1,2], [3,4], [5,6]])\n",
        "b = torch.rand(3,2)\n",
        "\n",
        "c = a + b\n",
        "\n",
        "print(\"a\", a)\n",
        "print(\"b\", b)\n",
        "print(\"c\", c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbmJvO6_FwCs"
      },
      "source": [
        "Elementwise Multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18JkwMVztzUn"
      },
      "outputs": [],
      "source": [
        "d = a * b\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upa4riBXF0Km"
      },
      "source": [
        "Changing shape of tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIcduRE68Q7T"
      },
      "source": [
        "Always check the shape of the tensors when performing complex operations. It's very useful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBLQJeAkt5PO"
      },
      "outputs": [],
      "source": [
        "e = torch.arange(6)\n",
        "print(\"Before:\")\n",
        "print(\"Shape\", e.shape)\n",
        "print(\"Tensor\", e)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Changing shape operation\n",
        "e = e.view(2,3)\n",
        "\n",
        "print(\"After:\")\n",
        "print(\"Shape\", e.shape)\n",
        "print(\"Tensor\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f5AI37ZF4iS"
      },
      "source": [
        "Matrix Multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UvJwP72uRoa"
      },
      "outputs": [],
      "source": [
        "print(a.shape)\n",
        "print(e.shape)\n",
        "\n",
        "# 3x2 matrix multiplied with 2x3 matrix should result in a 3x3 matrix\n",
        "\n",
        "f = torch.matmul(a,e)\n",
        "\n",
        "print(f.shape)\n",
        "print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mJKjFzt9dgw"
      },
      "source": [
        "Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOBrQfvy9flA"
      },
      "source": [
        "First we create a 2x2x4 tensor to see how we can access different parts of the tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "621D1c36uwrl"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(16)\n",
        "x = x.view(2, 2, 4)\n",
        "print(x)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJTcivu9_S3M"
      },
      "source": [
        "What does shape (2, 2, 4) mean exactly?\n",
        "\n",
        "We have:\n",
        "- 2 elements along the first dimension - 2 2D matrices;\n",
        "- 2 elements along the second dimension - 2 rows for each matrix;\n",
        "- 4 elements along the third dimension - 4 columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPcxb6hHvALl"
      },
      "outputs": [],
      "source": [
        "# Selecting the first element from the first dimension -> the first 2D matrix\n",
        "print(x[0])\n",
        "\n",
        "# ':' operator means select all elements from this dimension\n",
        "\n",
        "# Selecting all elements that have index 0 in second dimension -> first row of each 2D matrix\n",
        "print(x[:, 0])\n",
        "\n",
        "# Selecting all elements that have index 0 in third dimension -> all elements from first column\n",
        "print(x[:,:,0])\n",
        "\n",
        "# Selecting all elements that have index between 1 and 3 (excluding 3) in third dimension -> 2nd and 3rd column\n",
        "print(x[:,:,1:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV3aPrnuBJQf"
      },
      "source": [
        "Play with the indexing operations to get a more intuitive understanding of what they do.\n",
        "\n",
        "For the following tensor try to obtain this result with indexing:\n",
        "\n",
        "```\n",
        "tensor([[[ 6,  7],\n",
        "         [10, 11]],\n",
        "\n",
        "        [[22, 23],\n",
        "         [26, 27]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdO6afoMBQeZ"
      },
      "outputs": [],
      "source": [
        "x = torch.arange(64).view(4, 4, 4)\n",
        "print(x)\n",
        "\n",
        "# Use indexing the obtain the desired tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmelWkWBv1c5"
      },
      "source": [
        "#### Computation Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAa2AbTIxHFh"
      },
      "source": [
        "Gradient tracking refers to the ability to automatically compute gradients of a function with respect to its inputs or parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10HcIM1PxUEL"
      },
      "source": [
        "By default, tensors do not keep track of gradients. This can be checked with the `requires_grad` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_KOCJqHv4Qe"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([2.0, 3.0])\n",
        "x.requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBtzmFp0xik4"
      },
      "source": [
        "We can change this by setting `requires_grad` to `True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAn1M1cOxqKK"
      },
      "outputs": [],
      "source": [
        "# Setting requires_grad to True\n",
        "x.requires_grad_(True)\n",
        "print(x.requires_grad)\n",
        "\n",
        "# Initializing tensor with requires_grad True\n",
        "y = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "print(y.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVtM5SIsyzms"
      },
      "source": [
        "Notice the `grad_fn` attribute. It saves the last function that was performed on that tensor so it can compute its gradient (derivative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUg5r_VoydgK"
      },
      "outputs": [],
      "source": [
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "z = x * y\n",
        "print(z)\n",
        "\n",
        "z = x ** 2\n",
        "print(z)\n",
        "\n",
        "z = x.mean()\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Z-RjmAzIv1"
      },
      "source": [
        "We can compute gradients using the `backward()` function. We can check the gradients with the `.grad` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaVtALpAzFWM"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "z = y * y * 3\n",
        "z = z.mean()\n",
        "print(z)\n",
        "\n",
        "z.backward() # dz/dx\n",
        "print(\"Gradients of z with respect to x: \", x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqF49xpJ0Gji"
      },
      "source": [
        "#### GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQrqrDIP0IuA"
      },
      "source": [
        "For this part you need to select Runtime -> Change Runtime type -> Hardware accelerator: GPU and press save. You will need to rerun all the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAYo2TZf_W_P"
      },
      "source": [
        "Deep learning involves many operations that can be parallelized when working with tensors. Because of this, executing the computation on GPU leads to significantly faster processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vd_DPl20zye"
      },
      "source": [
        "You can check that GPU acceleration is available with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AgRT70s0371"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p08-re0F1M8E"
      },
      "source": [
        "We can specify the device on which all operations are done with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZcoV4kr1WFg"
      },
      "outputs": [],
      "source": [
        "# If GPU (cuda) is available then we use it, otherwise use CPU (not recommended)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z0srxh01ctr"
      },
      "source": [
        "Pushing a tensor on the GPU device is simply done with the `.to(...)` or `.cuda()` functions.\n",
        "\n",
        "All the operations performed with these tensors will be computed on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UkQcqB210tT"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([2.0, 3.0])\n",
        "x = x.to(device)\n",
        "print(x)\n",
        "\n",
        "y = torch.tensor([5.0, 1.0]).to(device)\n",
        "print(y)\n",
        "\n",
        "z = x + y\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixUgxdA03Z0D"
      },
      "source": [
        "# Part 2 - Training Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmLYgM667h3H"
      },
      "source": [
        "#### Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDw0Wq2G8E1X"
      },
      "source": [
        "Gradient descent is an optimization algorithm used to iteratively adjust model parameters based on the computed gradients, moving in the direction of steepest descent to find the optimal values that minimize the loss function.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/faq/gradient-optimization/ball.png\" width=400px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msv4hWK1HtLJ"
      },
      "source": [
        "Gradient descent can be described by the following formula:\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200611183120/1406-7.png\">\n",
        "\n",
        "where $\\theta$ are the weights (trainable parameters) of the model, $\\alpha$ is the learning rate and $J(\\theta)$ is the cost function (loss).\n",
        "\n",
        "The weights are updated in the opposite direction of the derivative (gradient) of the cost function. This basically slightly **modifies the parameters so that the loss decreases.**\n",
        "\n",
        "Remember that PyTorch can automatically track the gradients for us, which means we do not have to compute derivatives manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nZm1REx9Ojk"
      },
      "source": [
        "The steps of training a neural network are the following:\n",
        "\n",
        "1. Initialize parameters.\n",
        "2. Compute loss.\n",
        "3. Calculate gradients of loss with respect to the parameters.\n",
        "4. Update parameters by moving in the opposite direction of the gradients.\n",
        "5. Repeat steps 2-4 for multiple epochs.\n",
        "6. Stop when a stopping condition is met (reaching a desired loss or number of epochs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ2QAXZUIUO1"
      },
      "source": [
        "#### Training a Linear Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y57hYixT-zIn"
      },
      "source": [
        "We will train a very simple linear model to learn the simple function $f(x) = 2*x$ using gradient descent in order to get familiar with the training pipeline.\n",
        "\n",
        "The model will have a single parameter $w \\in R$ and the output of the model will be $pred(x) = w * x$.\n",
        "\n",
        "The model should learn that $w = 2$ if we show it enough examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36uQusRGI7yU"
      },
      "source": [
        "We create a few examples that the model will see. $x$ is the input and $y$ is the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-H7GxCeJFWs"
      },
      "outputs": [],
      "source": [
        "# Dummy dataset\n",
        "x = torch.tensor([1,2,3,4,5], dtype=torch.float32)\n",
        "y = torch.tensor([2,4,6,8,10], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Tgjd2RJII1"
      },
      "source": [
        "We define the **hyperparameters** of training the model. Hyperparameters are chosen by the developer and can be changed to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWx5qSLCJS5b"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I00SV5-jJWLE"
      },
      "source": [
        "We initialize the weights of the model ($w=0$) and define the forward function (how the model obtains the output based on the input)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBRw1i1qJs6A"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize parameters\n",
        "weights = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "# Define forward function for linear model - how the input is processed to obtain the prediction\n",
        "def forward(x):\n",
        "  return weights * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmAuJnuyJxYJ"
      },
      "source": [
        "We define the loss function $J$. This should tell us how close the model's predictions are to the ground truth.\n",
        "\n",
        "We choose mean absolute error as it is intuitive, but there are other loss functions. Most of them are already available in pytorch and we don't need to manually define them. You can check them [here](https://pytorch.org/docs/stable/nn.html#loss-functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdHnupniKXwq"
      },
      "outputs": [],
      "source": [
        "# Define the custom loss (mean absolute error)\n",
        "def custom_loss(pred, ground_truth):\n",
        "  return torch.abs(pred - ground_truth).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slxVRv3RKvTn"
      },
      "source": [
        "We want to see if the model will be able to predict the correct answer even for unseen samples. For example, $6$ is not in the samples in $x$. Right now, as the model has not been trained, its prediction is bad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsPMP-x_LIIQ"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  print(f\"Prediction before training: f(6) = {forward(6)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix3QustTLJlz"
      },
      "source": [
        "We perform the training loop:\n",
        "* we obtain the model's prediction\n",
        "* we see how good the predictions are based on the loss\n",
        "* we update the model's weights in the direction that minimizes the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwCRoRqD-ZxY"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  # Obtain prediction\n",
        "  pred = forward(x)\n",
        "\n",
        "  # 2. Compute loss\n",
        "  loss = custom_loss(pred, y)\n",
        "\n",
        "  # 3. Compute gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Gradient of loss with respect to weights\n",
        "  dw = weights.grad\n",
        "\n",
        "  # 4. Update parameters\n",
        "  # torch.no_grad tells it to not save gradients so it does not mess with gradient computation - this will be removed later when working with torch optimizers, so don't worry about it\n",
        "  with torch.no_grad():\n",
        "    weights -= (learning_rate * dw)\n",
        "\n",
        "  # Set gradient to 0 for next computation - they are not reset by default and will add up if you don't set them to 0\n",
        "  weights.grad.zero_()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, weights = {weights}, loss = {loss}\")\n",
        "\n",
        "print(f\"Prediction after training: f(6) = {forward(6)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9Xx90LgLk4u"
      },
      "source": [
        "Notice the final weight $w$ is not exactly equal to 2 but is very close.\n",
        "\n",
        "This can be improved with a smaller learning rate and maybe more training epochs.\n",
        "\n",
        "However, in machine learning we don't expect perfect solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucfPVuHuE-4g"
      },
      "source": [
        "**Exercise 1** - for the simple model above, change the hyperparameters and observe how the progress of weights changes. See if you can exactly obtain $w=2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXkqxcYOIh2f"
      },
      "source": [
        "#### Rewriting the code with modules from torch library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfWyZHSCFkWj"
      },
      "source": [
        "Luckily we do not have to manually perform the updates of the model.\n",
        "\n",
        "We can do this by defining the `optimizer = optim.SGD(...)` which does the gradient descent updates for us.\n",
        "\n",
        "Also, we do not need to manually define the weights. We can use `nn.Module` class and `nn.Linear(...)` for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFal4_O1IaN_"
      },
      "source": [
        "We construct a simple linear model by inheriting from `nn.Module` class.\n",
        "\n",
        "All neural networks you will build will inherit from this class. It's good to get familiar with it. You can check more details about it [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aykgJBFUIfiE"
      },
      "outputs": [],
      "source": [
        "class LinearModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LinearModel, self).__init__()\n",
        "    # One input neuron and one output neuron\n",
        "    self.weights = nn.Linear(in_features=1, out_features=1, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.weights(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR_NrriNJ1o"
      },
      "source": [
        "We initialize the model and define the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-f5SbfbNPBA"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize parameters\n",
        "model = LinearModel()\n",
        "print(model)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6yY8S9HNSuv"
      },
      "source": [
        "We initialize the optimizer. We will use `optim.SGD` which stands for Stochastic Gradient Descent. In the PyTorch library there are optimizers that may work better than Stochastic Gradient Descent (for example `optim.Adam` which is very commonly used). You can check them [here](https://pytorch.org/docs/stable/optim.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwsqZpdrNu7_"
      },
      "outputs": [],
      "source": [
        "# This will perform Gradient Descent for us\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8CUNRJBNx6L"
      },
      "source": [
        "The new model expects the samples to come in batches. We will add an extra dimension to our $x$ and $y$ tensors. The first dimension of these tensors will be the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL8wqhRlN7DN"
      },
      "outputs": [],
      "source": [
        "print(x.shape, y.shape)\n",
        "# Add extra dimension for batch:\n",
        "x = x.unsqueeze(1)\n",
        "y = y.unsqueeze(1)\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "# Define testing sample\n",
        "testing_sample = torch.tensor([[6.0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdEJFhHcOL63"
      },
      "source": [
        "Prediction on unseen sample before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSnpilYJOOJX"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  print(f\"Prediction before training: f(6) = {model(testing_sample)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIwPHcEuIxob"
      },
      "source": [
        "Now the training loop looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh_8eTWzHM5z"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "  # Obtain prediction\n",
        "\n",
        "  pred = model(x)\n",
        "\n",
        "  # 2. Compute loss\n",
        "  loss = custom_loss(pred, y)\n",
        "\n",
        "  # 3. Compute gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # 4. Update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  # Set gradient to 0 for next computation\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch}, loss = {loss}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(f\"Prediction after training: f(6) = {model(testing_sample)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhJaWFMMdAF4"
      },
      "source": [
        "#### Neural Network on Circle Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CnMuzE_3ftJ"
      },
      "source": [
        "We want to create more complex models - a single parameter model is very limited.\n",
        "\n",
        "We will build the following Neural Network with 2 input neurons, 3 neurons in the hidden layer and two output neurons:\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/289479445/figure/fig1/AS:614019022991383@1523404951564/Example-for-an-artificial-neural-network-with-two-input-neurons-two-hidden-neurons-and.png\" width=300px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50859qDqOVck"
      },
      "source": [
        "Each neuron in the neural network is just a linear model + an activation function (which adds nonlinearity).\n",
        "\n",
        "All the neurons in a previous layer are connected to all the layers in the current layer.\n",
        "\n",
        "This means that the output of a neuron will be computed based on all the outputs of the neurons in the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FIvJ9geJ_FH"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    # This defines the connections from input layer to hidden layer - 2 neruons to 3 neurons\n",
        "    self.fc1 = nn.Linear(in_features=2, out_features=3)\n",
        "\n",
        "    # This defines the connections from hidden layer to output layer - 3 neruons to 2 neurons\n",
        "    self.fc2 = nn.Linear(in_features=3, out_features=2)\n",
        "\n",
        "    # This is a commonly used nonlinearity\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "    # Activation functions which are more recent:\n",
        "    # self.activation = nn.GELU() - very popular, widely utilized in transformers\n",
        "    # self.activation = nn.Mish()\n",
        "    # self.activation = nn.ELU()\n",
        "\n",
        "    # Other activation functions (only used in specific cases):\n",
        "    # self.activation = nn.Tanh()\n",
        "    # self.activation = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # We pass through the hidden layer\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    # We add nonlinearity\n",
        "    x = self.activation(x)\n",
        "\n",
        "    # We pass through output layer\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a7G7fD03uzc"
      },
      "source": [
        "We will train it to classify samples consisting of points inside or outside a circle. We will have 2 classes: $inside=0$ and $outside=1$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8Ho7_M4MwcR"
      },
      "outputs": [],
      "source": [
        "# This is the code that generates the samples - you can ignore it\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define the circle parameters\n",
        "radius = 5\n",
        "center = (0, 0)\n",
        "\n",
        "# Generate the dataset\n",
        "num_samples = 2000\n",
        "\n",
        "# Generate random x, y points within a square\n",
        "x = torch.rand(num_samples) * 2 * radius - radius\n",
        "y = torch.rand(num_samples) * 2 * radius - radius\n",
        "\n",
        "# Calculate the Euclidean distance from each point to the center\n",
        "distances = torch.sqrt((x - center[0]) ** 2 + (y - center[1]) ** 2)\n",
        "\n",
        "# Assign labels based on whether points are inside or outside the circle\n",
        "labels = torch.where(distances <= radius, torch.zeros_like(distances), torch.ones_like(distances))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "labels_train, labels_test = train_test_split(labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Plot the training and testing datasets\n",
        "plt.scatter(x_train[labels_train == 0], y_train[labels_train == 0], color='blue', label='Inside Circle (Train)')\n",
        "plt.scatter(x_train[labels_train == 1], y_train[labels_train == 1], color='red', label='Outside Circle (Train)')\n",
        "plt.scatter(x_test[labels_test == 0], y_test[labels_test == 0], color='cyan', label='Inside Circle (Test)')\n",
        "plt.scatter(x_test[labels_test == 1], y_test[labels_test == 1], color='orange', label='Outside Circle (Test)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Dataset')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aHk-uIMOEfO"
      },
      "source": [
        "Create Dataset class which stores samples and their labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59Uq__pFODXf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CircleDataset(Dataset):\n",
        "    def __init__(self, x, y, labels):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    # Each sample will have the coords (2D vector) and a class label (inside or outside the circle)\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'coords': torch.tensor([self.x[idx], self.y[idx]]),\n",
        "            'label': self.labels[idx].long()\n",
        "        }\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2WPr9DIQFf5"
      },
      "source": [
        "The Dataset class in PyTorch represents a collection of data samples and labels. It is needed for the DataLoader class.\n",
        "\n",
        "The DataLoader class efficiently loads and manages the data during training or inference by providing features like batching and shuffling. These classes are commonly used in PyTorch. You can read more about them [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4PNm0VVOo-o"
      },
      "outputs": [],
      "source": [
        "train_dataset = CircleDataset(x_train, y_train, labels_train)\n",
        "test_dataset = CircleDataset(x_test, y_test, labels_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chKFjo9rtOEn"
      },
      "source": [
        "We will define an evaluation function which calculates the accuracy on the test set. This function will be used in the training loop in order to keep track of the evolution of the model on unseen data.\n",
        "\n",
        "We compute the accuracy manually for educational purposes. However, this is not necessary. There are already functions for computing evaluation metrics in the sklearn library. You can check them [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics). See [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) and [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U8zc2bGKIvP"
      },
      "source": [
        "Additional Note: Notice that when evaluating we do not keep track of gradients. We only want to keep track of gradients when training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVPsSeVys_D9"
      },
      "outputs": [],
      "source": [
        "def evaluate(model):\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # For keeping track of number of correct predictions and total predictions\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for data in test_loader:\n",
        "      coords = data['coords'].to(device)\n",
        "      ground_truth = data['label'].to(device)\n",
        "\n",
        "      # We do not need to keep track of gradients while testing\n",
        "      with torch.no_grad():\n",
        "        pred = model(coords)\n",
        "\n",
        "      # For each sample the model predicts 2 values - score for inside class and score for outside class\n",
        "      # We keep the prediction that has the highest score\n",
        "      _, predicted = torch.max(pred, 1)\n",
        "\n",
        "      # We count all the predictions which match the ground truth to get number of correct predictions\n",
        "      correct += (predicted == ground_truth).sum().item()\n",
        "      total += coords.shape[0]\n",
        "\n",
        "  accuracy = np.round(100 * correct / total, 2)\n",
        "\n",
        "  print(f\"Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmWo5gO6dMaa"
      },
      "source": [
        "The training loop will look very similar to the previous example.\n",
        "\n",
        "However, now we also iterate through the data loader which returns batches of random samples and their labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-0Hp2cuQStQ"
      },
      "outputs": [],
      "source": [
        "# Cross Entropy Loss - commonly used for classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "epochs = 10\n",
        "\n",
        "# 1. Initialize parameters\n",
        "model = Net()\n",
        "\n",
        "# Push model on device\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0\n",
        "  for data in train_loader:\n",
        "    # Put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    coords = data['coords'].to(device)\n",
        "    ground_truth = data['label'].to(device)\n",
        "    pred = model(coords)\n",
        "\n",
        "    # 2. Compute loss\n",
        "    loss = criterion(pred, ground_truth)\n",
        "\n",
        "    # 3. Compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 4. Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Set gradient to 0 for next computation\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    epoch_loss += loss\n",
        "\n",
        "  print(f\"Epoch: {epoch}, Training loss = {epoch_loss}\")\n",
        "  evaluate(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jp-fa2qaxzS"
      },
      "source": [
        "**Exercise 2** - Play with the hyperparameters (learning rate, number of epochs) and with the structure of the model (add more neurons in the hidden layer, add more layers) and see if you can improve the results. Maybe also change the optimizer.\n",
        "\n",
        "Try to apply only one change at a time and test it to see what improves the results and what decreases performance.\n",
        "\n",
        "Do not perform many changes at once as you will not know what caused a change in performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vt-0U8eUoes"
      },
      "source": [
        "#### Neural Network for classification on the Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dxIzyT_iXlB"
      },
      "source": [
        "**Exercise 3** - Train a Neural Network for classification on the iris dataset.\n",
        "\n",
        "The iris dataset consists of measurements of sepal and petal lengths and widths for three different species of iris flowers: setosa, versicolor, and virginica. See more details about the dataset [here](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) and [here](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
        "\n",
        "Your input to the model will be all these measurements (features) and you will have 3 flower classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTTtByaoi6Ut"
      },
      "source": [
        "This code imports the dataset and creates the dataloaders that you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0d55VnkdWat"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "data = iris.data\n",
        "labels = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roc52UG3kIor"
      },
      "source": [
        "Plotting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBFMs_FRkHA9"
      },
      "outputs": [],
      "source": [
        "# Create plots for different feature combinations\n",
        "feature_combinations = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
        "num_plots = len(feature_combinations)\n",
        "num_rows = (num_plots + 1) // 3  # Calculate the number of rows required for the subplot grid\n",
        "\n",
        "# Create the subplot grid\n",
        "fig, axs = plt.subplots(num_rows, 3, figsize=(10, num_rows * 3))\n",
        "\n",
        "# Plotting the dataset for each feature combination\n",
        "for i, (feat_idx1, feat_idx2) in enumerate(feature_combinations):\n",
        "    row = i // 3  # Determine the row index in the subplot grid\n",
        "    col = i % 3   # Determine the column index in the subplot grid\n",
        "    axs[row, col].scatter(data[:, feat_idx1], data[:, feat_idx2], c=labels, cmap='viridis')\n",
        "    axs[row, col].set_xlabel(feature_names[feat_idx1])\n",
        "    axs[row, col].set_ylabel(feature_names[feat_idx2])\n",
        "    axs[row, col].set_title(f\"{feature_names[feat_idx1]} vs {feature_names[feat_idx2]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnydQ49XSg_Q"
      },
      "source": [
        "We create the Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRMstEb5SfBp"
      },
      "outputs": [],
      "source": [
        "class IrisDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'features': torch.tensor(self.features[idx], dtype=torch.float32),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnt1h_2MSoqm"
      },
      "source": [
        "We create the dataloaders for training set and testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK2s5-EhSanq"
      },
      "outputs": [],
      "source": [
        "# Create train and test datasets\n",
        "iris_train_dataset = IrisDataset(x_train, y_train)\n",
        "iris_test_dataset = IrisDataset(x_test, y_test)\n",
        "\n",
        "# Create train and test data loaders\n",
        "iris_train_loader = DataLoader(iris_train_dataset, batch_size=64, shuffle=True)\n",
        "iris_test_loader = DataLoader(iris_test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjGmYSRGOdXe"
      },
      "outputs": [],
      "source": [
        "# Check out some samples from the dataset\n",
        "print(iris_train_dataset[0])\n",
        "print(iris_train_dataset[50])\n",
        "print(iris_train_dataset[110])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Svg0tNyjD1B"
      },
      "source": [
        "Your job is to construct the neural network and write the training and evaluation.\n",
        "\n",
        "Play with the number of hidden layers and their size (number of neurons) to see what configuration is best.\n",
        "\n",
        "Some things to keep in mind:\n",
        "- What is the dimensionality of the input? This will tell you the number of input neurons for first layer\n",
        "- What is the number of classes? This will tell you the number of output neurons for final layer\n",
        "- When adding multiple hidden layers the in_features (number of neurons) of current layer should match the out_features of the previous layer\n",
        "- Do not forget about using activation functions after each layer (except output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI75fFt4drSt"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, input_size=128, hidden_size=64, output_size=10):\n",
        "        super(MyNet, self).__init__()\n",
        "        \n",
        "        # First fully connected layer\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # Second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        # Output layer\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        # Dropout (to prevent overfitting)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer with ReLU activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # Second layer with ReLU and dropout\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        # Output layer with softmax activation\n",
        "        x = F.log_softmax(self.fc3(x), dim=1)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = MyNet(input_size=128, hidden_size=64, output_size=10)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib7Km8EejG5P"
      },
      "source": [
        "Write the training loop and the evaluation:\n",
        "\n",
        "* Define the hyperparameters\n",
        "* Define the loss function (criterion) - You can use already existing ones.\n",
        "* Define the optimizer\n",
        "* Initialize the model and train it\n",
        "* Evaluate the model's accuracy\n",
        "\n",
        "You can take inspiration from the previous example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_YFJE7SeB97"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "input_size = 128\n",
        "hidden_size = 64\n",
        "output_size = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "X_train = torch.randn(5000, input_size)             # 5000 samples, each with 128 features\n",
        "y_train = torch.randint(0, output_size, (5000,))    # 5000 labels in range 0-9\n",
        "\n",
        "X_test = torch.randn(1000, input_size)              # 1000 test samples\n",
        "y_test = torch.randint(0, output_size, (1000,))     # 1000 test labels\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = MyNet(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct/total:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMS5awVmYRMZz8t7vurc80E",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
