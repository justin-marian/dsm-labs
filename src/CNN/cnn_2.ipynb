{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyCatruna/DSM/blob/main/Lab_03_CNN_Architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo1ztV4wMm7H"
      },
      "source": [
        "# CNN Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xScZXeqMqHk"
      },
      "source": [
        "#### Motivation\n",
        "In the previous Lab we saw that it is difficult to construct a custom CNN for specific tasks as we have to experiment with many hyperparameters (number of layers, number of feature maps per layer, kernel sizes, fully connected network configuration, etc.)\n",
        "\n",
        "Luckily we do not have to do that. There are plenty of architectures that have been constructed and tested for many tasks.\n",
        "\n",
        "With very minor changes, we can adapt existing architectures to our task and be relatively confident that it will obtain good results (if trained correctly).\n",
        "\n",
        "In Lab 3 you will learn how to utilize available architectures and how to fine-tune them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOqN0dbL629N"
      },
      "source": [
        "#### Notes\n",
        "As we utilize deep networks, training will take quite a long time in this lab. Running on GPU is necessary (Runtime -> Change Runtime Type).\n",
        "\n",
        "If you see that a run is not going anywhere (Validation Accuracy is not increasing or Loss is not decreasing) cancel it to save some time.\n",
        "\n",
        "If possible, run the training with higher batch sizes to speed up the training.\n",
        "\n",
        "While the models are training, we recommend that you look over the papers of the architectures.ðŸ˜„"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z54LucfQwgJ"
      },
      "source": [
        "#### Libraries\n",
        "There are plenty of architectures available in the [torchvision library](https://pytorch.org/vision/0.8/models.html), the [timm library](https://timm.fast.ai/), and on [Hugging Face](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPMMdBmMeaJ2"
      },
      "source": [
        "### Training an off-the-shelf model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNuZ77zgLXAW"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRbe3lPneiBK"
      },
      "source": [
        "We will use the CIFAR10 dataset as in the previous lab. Let's see if we can obtain better results this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGJPOIhDeg8r"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "\n",
        "# The batch size is the number of images the model processes in parallel\n",
        "# We use shuffle for training as we don't want the model to see the images in the same order\n",
        "trainloader = DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "\n",
        "# For testing we don't have to shuffle the data\n",
        "testloader = DataLoader(testset, batch_size=256,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksE1R1Sdev_s"
      },
      "source": [
        "Code for training and validation - same as in previous lab\n",
        "\n",
        "You should already be familiar with the following blocks of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_24CNaNHe2DK"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4j81PNAe3oC"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, device, optimizer, criterion, epoch):\n",
        "    # We set the model to be in training mode\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    # We iterate through all batches - 1 step is 1 batch of batch_size images\n",
        "    for _, (images, labels) in bar:\n",
        "        # We take the images and their labels and push them on GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Obtain predictions\n",
        "        pred = model(images)\n",
        "\n",
        "        # Compute loss for this batch\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        # Compute gradients for each weight (backpropagation)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights based on gradients (gradient descent)\n",
        "        optimizer.step()\n",
        "\n",
        "        # We keep track of the average training loss\n",
        "        total_train_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFcj3zd2e6Gk"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(model, dataloader, device, criterion, epoch):\n",
        "    # We set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    # We keep track of correct predictions\n",
        "    correct = 0\n",
        "\n",
        "    # This is only for showing the progress bar\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "\n",
        "    for step, (images, labels) in bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, labels)\n",
        "\n",
        "        # The raw output of the model is a score for each class\n",
        "        # We keep the index of the class with the highest score as the prediction\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "\n",
        "        # We see how many predictions match the ground truth labels\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # We compute evaluation metrics - loss and accurarcy\n",
        "        total_val_loss += (loss.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_val_loss / dataset_size, 2)\n",
        "\n",
        "        accuracy = np.round(100 * correct / dataset_size, 2)\n",
        "\n",
        "        bar.set_postfix(Epoch=epoch, Valid_Acc=accuracy, Valid_Loss=epoch_loss)\n",
        "\n",
        "    return accuracy, epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0KULbF2e9Nk"
      },
      "outputs": [],
      "source": [
        "def run_training(model, num_epochs, learning_rate):\n",
        "    # Define criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Check if we are using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    # For keeping track of the best validation accuracy\n",
        "    top_accuracy = 0.0\n",
        "\n",
        "    # We train the emodel for a number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, criterion, epoch)\n",
        "        print(f\"Epoch {epoch} - Training Loss: {train_loss}\")\n",
        "\n",
        "        # For validation we do not keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            val_accuracy, _ = valid_epoch(model, testloader, device, criterion, epoch)\n",
        "            if val_accuracy > top_accuracy:\n",
        "                print(f\"Validation Accuracy Improved ({top_accuracy} ---> {val_accuracy})\")\n",
        "                top_accuracy = val_accuracy\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfjRk3fwfAJy"
      },
      "source": [
        "Code for counting number of parameters of a model.\n",
        "\n",
        "As we have limited compute resources, we will opt for models with fewer parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q8glR5ufPXg"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwfSmbgDjiXu"
      },
      "source": [
        "### SqueezeNet from torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeQy1M-6fgdy"
      },
      "source": [
        "First, let's experiment with a model from the torchvision library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVGd4_eDffl5"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdVMox4of9v9"
      },
      "source": [
        "Let's see what models are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVPFefxWf_qC"
      },
      "outputs": [],
      "source": [
        "models.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr8b9HO7gSha"
      },
      "source": [
        "We will choose SqueezeNet which is a very lightweight model. More details about the architecture can be found in this [paper](https://arxiv.org/abs/1602.07360).\n",
        "\n",
        "<img src=https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-26_at_6.04.32_PM.png width=750px>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNDDxD9ugVY3"
      },
      "outputs": [],
      "source": [
        "squeeze_net = models.squeezenet1_0()\n",
        "print(squeeze_net)\n",
        "print(\"Number of parameters:\", count_parameters(squeeze_net))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4amQ6X2oihM7"
      },
      "source": [
        "Visualization of modifying prediction head. In our case, we only change the final output layer.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/0*8Z3To8OAwBBIj66p.jpg\" width=700px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bnAUzvhhaqb"
      },
      "source": [
        "The classifier is built for 1000 classes as it was intended for training on ImageNet. We have to modify it to work on 10 classes. Notice that the number of parameters changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYAl9idzhkyy"
      },
      "outputs": [],
      "source": [
        "squeeze_net.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1,1), stride=(1,1))\n",
        "squeeze_net.to(device)\n",
        "print(\"Number of parameters:\", count_parameters(squeeze_net))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IREYZYpiF0Sg"
      },
      "source": [
        "We run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78uoDbL0h_RU"
      },
      "outputs": [],
      "source": [
        "# You may want to change the hyperparameters to obtain better results\n",
        "run_training(model=squeeze_net, num_epochs=10, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILDTtPWMjOmY"
      },
      "source": [
        "### Fine-tuning pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mmKqF5GjxpE"
      },
      "source": [
        "The performance is not that impressive. You could improve the results by playing with the learning rate, number of epochs, and add image augmentations.\n",
        "\n",
        "However, we do not have to start from randomly initialized weights.\n",
        "\n",
        "We can start from weights pre-trained on large datasets (ImageNet) and fine-tune the weights on the new task.\n",
        "\n",
        "This is simply done by specifying :```pretrained=True``` when initializing the model\n",
        "\n",
        "This should improve the performance as the convolutional layers have already learnt to extract meaningful features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVFJIMqwj6Hv"
      },
      "outputs": [],
      "source": [
        "squeeze_net = models.squeezenet1_0(pretrained=True)\n",
        "squeeze_net.classifier[1] = nn.Conv2d(512, 10, kernel_size=(1,1), stride=(1,1))\n",
        "squeeze_net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80bdocJ5kWzY"
      },
      "outputs": [],
      "source": [
        "# Usually when we fine-tune pretrained models we utilize lower learning rates\n",
        "run_training(model=squeeze_net, num_epochs=10, learning_rate=0.0003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV2zfrYI629Q"
      },
      "source": [
        "The model should obtain better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MixNQlhUjvoM"
      },
      "source": [
        "### Resnet-18 from timm library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N14ES_v-m_zl"
      },
      "source": [
        "Let's try another model from a different library. We will use the timm library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3okAqrq6nF57"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXnc0dzJnKax"
      },
      "outputs": [],
      "source": [
        "import timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gseg32BHnL-Y"
      },
      "source": [
        "Let's see available models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFC8BrThnNnu"
      },
      "outputs": [],
      "source": [
        "timm.list_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBDfA1APGi5y"
      },
      "source": [
        "We will train a ResNet-18 that is already pre-trained on ImageNet.\n",
        "You can read more about ResNet in this [paper](https://arxiv.org/abs/1512.03385).\n",
        "\n",
        "<img src=https://www.researchgate.net/publication/366608244/figure/fig1/AS:11431281109643320@1672145338540/Structure-of-the-Resnet-18-Model.jpg>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA0MnrJInTx0"
      },
      "outputs": [],
      "source": [
        "resnet = timm.create_model('resnet18', pretrained=True)\n",
        "print(resnet)\n",
        "print(\"Number of parameters:\", count_parameters(resnet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc5Wy-UsnlFG"
      },
      "source": [
        "Similarly, we will need to modify the classifier to have only 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzIpaqLqnq3w"
      },
      "outputs": [],
      "source": [
        "resnet.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "resnet.to(device)\n",
        "print(resnet)\n",
        "print(\"Number of parameters:\", count_parameters(resnet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cxQ2Oor629Q"
      },
      "source": [
        "We run the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dttn92Gzn3jo"
      },
      "outputs": [],
      "source": [
        "# You may have to play with the hyperparameters to obtain better results\n",
        "run_training(model=resnet, num_epochs=10, learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgouVJX6j10_"
      },
      "source": [
        "### MobileNet from HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMOfmvbQIMSs"
      },
      "source": [
        "Finally, we will use a model from the Hugging Face library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM7l7K-19L3q"
      },
      "source": [
        "We will use the MobileNet architecture. You can read more about it in this [paper](https://arxiv.org/abs/1704.04861)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpAZ-2P_o2XX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate accelerate pillow torchvision scikit-learn\n",
        "from transformers import AutoModelForImageClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2mFNcC1JU-s"
      },
      "source": [
        "We utilize `AutoModelForImageClassification.from_pretrained`to create the model.\n",
        "As in the other cases, we change the classifier to have only 10 classes.\n",
        "\n",
        "We build a wrapper around the model to only output the score for each class (by default it also outputs the loss). We only do this so it works with our train and valid functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnNHsyn7o5cz"
      },
      "outputs": [],
      "source": [
        "class MobileNetWrapper(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MobileNetWrapper, self).__init__()\n",
        "    self.model = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n",
        "\n",
        "    # Changing the classifier to output only 10 classes\n",
        "    self.model.classifier = nn.Linear(in_features=1280, out_features=10, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x).logits\n",
        "\n",
        "mobilenet = MobileNetWrapper()\n",
        "mobilenet.to(device)\n",
        "print(\"Number of parameters:\", count_parameters(mobilenet))\n",
        "print(mobilenet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ksgfJ5Apib5"
      },
      "outputs": [],
      "source": [
        "# You may have to play with the hyperparameters to obtain better results\n",
        "run_training(model=mobilenet, num_epochs=10, learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Vba_tdL161"
      },
      "source": [
        "### Training only a subset of layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSJyPLsvMrRm"
      },
      "source": [
        "When using pre-trained models that were trained on similar images, we do not have to re-train every layer of the network.\n",
        "\n",
        "Ideally the first layers of the network already extract meaningful features which can be utilized in our task. So, in theory we don't have to re-train these first layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idIVzmyM629R"
      },
      "source": [
        "We will demonstrate this by fine-tuning only a part of the resnet-18 model. Let's have a closer look at the structure of ResNet18."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOmFCDqT629R"
      },
      "outputs": [],
      "source": [
        "resnet = timm.create_model('resnet18', pretrained=True)\n",
        "\n",
        "# Change the output layer to match the number of classes in CIFAR-10\n",
        "resnet.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
        "\n",
        "resnet.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83L2CxEN629R"
      },
      "source": [
        "<img src=https://www.researchgate.net/publication/366608244/figure/fig1/AS:11431281109643320@1672145338540/Structure-of-the-Resnet-18-Model.jpg>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClWOi53l629R"
      },
      "source": [
        "The model has an initial 7x7 Conv, followed by a Pooling Layer.\n",
        "\n",
        "Afterwards, it has 4 layers, each with the same structure. Finally, it has a fully connected classifier (which we changed to output only 10 scores instead of 1000).\n",
        "\n",
        "A layer is composed of 2 Building Blocks. A building Block contains the following:\n",
        "1. A 3x3 Conv\n",
        "2. Batch Norm - read more about this operation [here](https://arxiv.org/abs/1502.03167)\n",
        "3. Activation - ReLU\n",
        "4. A second 3x3 Conv\n",
        "5. Batch Norm\n",
        "6. Activation - ReLU\n",
        "7. Residual Connection with the Input - input + output - read more about this [here](https://arxiv.org/abs/1512.03385)\n",
        "\n",
        "A Building Block with residual connection looks like this:\n",
        "\n",
        "<img src=https://miro.medium.com/v2/resize:fit:570/1*D0F3UitQ2l5Q0Ak-tjEdJg.png width=300px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8e_vFtR629R"
      },
      "source": [
        "We will start by freezing all the parameters of the model. This means that we tell pytorch not to compute gradients for these parameters.\n",
        "\n",
        "Any parameter that is frozen will not be changed during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsxVUSkW629R"
      },
      "outputs": [],
      "source": [
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKjMRxMc629R"
      },
      "source": [
        "Function that prints the name of the parameter and if it's trainable or frozen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyGVlGrO629R"
      },
      "outputs": [],
      "source": [
        "def print_params(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzGdxUzb629R"
      },
      "outputs": [],
      "source": [
        "print_params(resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HesC9Gmm629R"
      },
      "source": [
        "All the parameters of the model are frozen.\n",
        "\n",
        "We will unfreeze only the final fc layer.\n",
        "\n",
        "We usually freeze the first layers and fine-tune the later ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBxy6aRW629R"
      },
      "outputs": [],
      "source": [
        "for param in resnet.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "print_params(resnet)\n",
        "print(\"Number of trainable parameters:\", count_parameters(resnet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6q7jTbV629S"
      },
      "source": [
        "Notice that we only have 5130 trainable parameters out of 11M.\n",
        "Let's run the training and see if the model learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOHbxniK629S"
      },
      "outputs": [],
      "source": [
        "# You may have to play with the hyperparameters\n",
        "run_training(model=resnet, num_epochs=20, learning_rate=0.0003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-h3D3Kc629S"
      },
      "source": [
        "**Exercise 1** - Unfreeze all the BatchNorm (bn) weights and biases and re-train the network. See what you observe.\n",
        "\n",
        "- Play with freezing and unfreezing certain parts of the network. See if you can obtain good results without having to re-train too many parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unfreeze BatchNorm layers\n",
        "for name, param in resnet.named_parameters():\n",
        "    if \"bn\" in name: \n",
        "        param.requires_grad = True\n",
        "\n",
        "print_params(resnet)\n",
        "print(\"Number of trainable parameters:\", count_parameters(resnet))\n",
        "run_training(model=resnet, num_epochs=20, learning_rate=0.0003)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7A6ySYm629S"
      },
      "source": [
        "**Exercise 2** - Try to obtain the highest accuracy possible on CIFAR-10.\n",
        "- Use any model you want from any of the presented libraries\n",
        "- Use image augmentations in training and try to find optimal hyperparameters\n",
        "- Models trained on ImageNet were trained with 224x224 images. What happens if you resize the CIFAR-10 32x32 images to the 224x224 size? (Training will take a very long time if you do this, but try it for 1-2 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "resnet50 = timm.create_model('resnet50', pretrained=True)\n",
        "resnet50.fc = nn.Linear(in_features=2048, out_features=10)\n",
        "resnet50.to(device)\n",
        "\n",
        "run_training(model=resnet50, num_epochs=15, learning_rate=0.0003)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
