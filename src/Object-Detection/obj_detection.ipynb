{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyCatruna/DSM/blob/main/Lab_05b_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmxQIhA8n4X_"
      },
      "source": [
        "## Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inuTMw46gbw6"
      },
      "source": [
        "Object detection refers to the task of predicting the bounding boxes and classes of the objects in the image.\n",
        "\n",
        "In this lab we will train and evaluate a small object detection model for educational purposes.\n",
        "\n",
        "However, if you want to utilize efficient pre-trained models for detection or fine-tune them, we recommend checking out [ultralytics](https://github.com/ultralytics/ultralytics) which makes inference and training very easy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRz579w_ghZ6"
      },
      "source": [
        "<img src=\"https://visionplatform.ai/wp-content/uploads/2024/01/object-detection.png\" widht=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL4iGbyvgnCY"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchmetrics pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6FYvwz5oFsI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torchvision.ops import nms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVlTVK-hIEKb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNAv4T3QyyIw"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z2V53Q3gM0Z"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "            \"aeroplane\": 1, \"bicycle\": 2, \"bird\": 3, \"boat\": 4, \"bottle\": 5,\n",
        "            \"bus\": 6, \"car\": 7, \"cat\": 8, \"chair\": 9, \"cow\": 10,\n",
        "            \"diningtable\": 11, \"dog\": 12, \"horse\": 13, \"motorbike\": 14, \"person\": 15,\n",
        "            \"pottedplant\": 16, \"sheep\": 17, \"sofa\": 18, \"train\": 19, \"tvmonitor\": 20\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVn2__AFgumq"
      },
      "source": [
        "We will use the [Pascal VOC Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html) for this part of the lab as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPAN-U3XgM0a"
      },
      "outputs": [],
      "source": [
        "''' Wrapper over the dataset in which we resize the bounding boxes to our image sizes '''\n",
        "class VOCDetectionDataset(VOCDetection):\n",
        "    def __init__(self, root, image_set, transform=None, download=False):\n",
        "        super().__init__(root=root, image_set=image_set, download=download)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        self.num_classes = 21\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, target = super().__getitem__(idx)\n",
        "\n",
        "        # Get original image dimensions\n",
        "        orig_width = int(target[\"annotation\"][\"size\"][\"width\"])\n",
        "        orig_height = int(target[\"annotation\"][\"size\"][\"height\"])\n",
        "\n",
        "        # Convert image to tensor and resize\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Adjust bounding boxes for the resized image\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for obj in target[\"annotation\"][\"object\"]:\n",
        "            bndbox = obj[\"bndbox\"]\n",
        "            xmin = int(bndbox[\"xmin\"])\n",
        "            ymin = int(bndbox[\"ymin\"])\n",
        "            xmax = int(bndbox[\"xmax\"])\n",
        "            ymax = int(bndbox[\"ymax\"])\n",
        "\n",
        "            # Scale bounding boxes\n",
        "            xmin = xmin * 128 / orig_width\n",
        "            ymin = ymin * 128 / orig_height\n",
        "            xmax = xmax * 128 / orig_width\n",
        "            ymax = ymax * 128 / orig_height\n",
        "\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            class_name = obj[\"name\"]\n",
        "            labels.append(label_map[class_name])\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # Create the target dictionary\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": torch.tensor([idx])\n",
        "        }\n",
        "\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wROQ4KRyg5Lo"
      },
      "source": [
        "We will work with only a subset of the dataset, so the training does not take too long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emvRAsG-gM0c"
      },
      "outputs": [],
      "source": [
        "''' Code for obtaining the dataset '''\n",
        "\n",
        "# As we deal with variable lengths for the targets (variable number of objects) we can't stack labels into a tensor\n",
        "# We use collate_fn to stack the images and targets into separate tuples\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = zip(*batch)\n",
        "    return images, targets\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "train_dataset = VOCDetectionDataset(\n",
        "    root='./data',\n",
        "    image_set='train',\n",
        "    download=True,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "# Obtain training subset\n",
        "train_dataset = torch.utils.data.Subset(train_dataset, np.random.choice(len(train_dataset), 2000))\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_dataset = VOCDetectionDataset(\n",
        "    root='./data',\n",
        "    image_set='val',\n",
        "    download=True,\n",
        "    transform=test_transform,\n",
        ")\n",
        "\n",
        "# Obtain testing subset\n",
        "test_dataset = torch.utils.data.Subset(test_dataset, np.random.choice(len(test_dataset), 200))\n",
        "\n",
        "testloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZqS8fTzgM0d"
      },
      "outputs": [],
      "source": [
        "''' Code for visualizing the ground truth bounding boxes and predicted bounding boxes '''\n",
        "\n",
        "def visualize_images_and_boxes(images, targets, predictions=None, num_samples=5, confidence_threshold=0.3, iou_threshold=0.5):\n",
        "    num_samples = min(num_samples, len(images))\n",
        "    num_cols = 2 if predictions is not None else 1\n",
        "    fig, axs = plt.subplots(num_samples, num_cols, figsize=(8, 4 * num_samples))\n",
        "\n",
        "    for i, (image, target) in enumerate(zip(images[:num_samples], targets[:num_samples])):\n",
        "        # De-normalize the image\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        image = image * std + mean\n",
        "        image = torch.clamp(image, 0, 1)\n",
        "\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "        boxes = target[\"boxes\"].cpu().numpy()\n",
        "        labels = target[\"labels\"].cpu().numpy()\n",
        "\n",
        "        ax_gt = axs[i]\n",
        "        if num_cols == 2:\n",
        "            ax_gt = axs[i, 0]\n",
        "        ax_gt.imshow(image_np)\n",
        "        ax_gt.set_title(\"Ground Truth\")\n",
        "\n",
        "\n",
        "        for box, label in zip(boxes, labels):\n",
        "            x_min, y_min, x_max, y_max = box\n",
        "            width, height = x_max - x_min, y_max - y_min\n",
        "\n",
        "            rect = plt.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax_gt.add_patch(rect)\n",
        "\n",
        "            label_name = [k for k, v in label_map.items() if v == label][0]  # Convert label index to name\n",
        "            ax_gt.text(x_min, y_min - 5, label_name, color='red', fontsize=12, weight='bold', backgroundcolor='white')\n",
        "\n",
        "        ax_gt.axis('off')\n",
        "\n",
        "        if predictions is not None:\n",
        "            # Filter predictions with low scores\n",
        "            keep = predictions[i][\"scores\"] > confidence_threshold\n",
        "            predictions[i][\"boxes\"] = predictions[i][\"boxes\"][keep]\n",
        "            predictions[i][\"labels\"] = predictions[i][\"labels\"][keep]\n",
        "            predictions[i][\"scores\"] = predictions[i][\"scores\"][keep]\n",
        "\n",
        "            # Apply non-maximum suppression\n",
        "            keep = nms(predictions[i][\"boxes\"], predictions[i][\"scores\"], iou_threshold=iou_threshold)\n",
        "            predictions[i][\"boxes\"] = predictions[i][\"boxes\"][keep]\n",
        "            predictions[i][\"labels\"] = predictions[i][\"labels\"][keep]\n",
        "            predictions[i][\"scores\"] = predictions[i][\"scores\"][keep]\n",
        "\n",
        "            pred_boxes = predictions[i][\"boxes\"].cpu().numpy()\n",
        "            pred_labels = predictions[i][\"labels\"].cpu().numpy()\n",
        "            pred_scores = predictions[i][\"scores\"].cpu().numpy()\n",
        "\n",
        "            ax_pred = axs[i, 1]\n",
        "            ax_pred.imshow(image_np)\n",
        "            ax_pred.set_title(\"Predictions\")\n",
        "\n",
        "            for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
        "                x_min, y_min, x_max, y_max = box\n",
        "                width, height = x_max - x_min, y_max - y_min\n",
        "\n",
        "                rect = plt.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
        "                ax_pred.add_patch(rect)\n",
        "\n",
        "                label_name = [k for k, v in label_map.items() if v == label][0]\n",
        "                ax_pred.text(x_min, y_min - 5, f\"{label_name} ({score:.2f})\", color='green', fontsize=12, weight='bold', backgroundcolor='white')\n",
        "\n",
        "            ax_pred.axis('off')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2wgkT00gM0f"
      },
      "outputs": [],
      "source": [
        "# Visualize 5 images and bounding boxes from the train_loader\n",
        "images, targets = next(iter(trainloader))\n",
        "\n",
        "visualize_images_and_boxes(images, targets, num_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOcZ1gzPo2R8"
      },
      "source": [
        "We will train a Single-Shot-Detector (SSD) architecture that uses a MobileNet_v3 backbone.\n",
        "\n",
        "Details about the backbone can be found [here](https://arxiv.org/abs/1905.02244)\n",
        "\n",
        "The backbone is tasked with extracting relevant features.\n",
        "\n",
        "Details about SSD can be found [here](https://arxiv.org/abs/1512.02325)\n",
        "\n",
        "**High-Level Overview of Object Detection**\n",
        "\n",
        "The detection takes as input the feature maps extracted by the backbone and outputs the bounding box predictions along with class scores.\n",
        "\n",
        "It works by dividing the image into a grid cell. Each grid cell is tasked with predicting the object that has the center inside it.\n",
        "\n",
        "For each grid cell the model predicts a number of bounding boxes, along with the confidence and class scores.\n",
        "\n",
        "A lot of predicted bounding boxes will have no object inside it - the model will predict background class for that prediction.\n",
        "\n",
        "A lot of predicted bounding boxes will be overlapping on the same object - we will use Non-Maximum Supression (NMS) to eliminate overlapping predictions and keep the one with the highest confidence. You can read more about NMS [here](https://medium.com/analytics-vidhya/non-max-suppression-nms-6623e6572536)\n",
        "\n",
        "\n",
        "<img src=\"https://iq.opengenus.org/content/images/2021/12/1_St98vVQEqLndeV_-SeUc9Q.png\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHoixS9on6jI"
      },
      "outputs": [],
      "source": [
        "ssd_lite = ssdlite320_mobilenet_v3_large(num_classes=21, pretrained_backbone=True, trainable_backbone_layers=2)\n",
        "\n",
        "ssd_lite.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt2mzOQQgM0h"
      },
      "outputs": [],
      "source": [
        "count_parameters(ssd_lite)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwm1AR6XtxDA"
      },
      "source": [
        "The model computes its losses intrnally based on the number of objects in the image. It trains for both localization and classification. We obtain a list of losses which we use to train the model.\n",
        "\n",
        "For validation we filter predictions below a confidence score and apply NMS to remove overlapping boxes.\n",
        "\n",
        "We utilize Mean Average Precision to evaluate the model. You can read more about it [here](https://towardsdatascience.com/what-is-average-precision-in-object-detection-localization-algorithms-and-how-to-calculate-it-3f330efe697b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebRj6DXhgM0i"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, device, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    dataset_size = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, targets) in bar:\n",
        "        images = [img.to(device) for img in images]\n",
        "        images = torch.stack(images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        batch_size = len(images)\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # We keep track of the average training loss\n",
        "        total_train_loss += (losses.item() * batch_size)\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = np.round(total_train_loss / dataset_size, 2)\n",
        "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss)\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "# Validation Function\n",
        "def valid_epoch(model, dataloader, device, iou_threshold=0.5, score_threshold=0.1):\n",
        "    model.eval()\n",
        "    metric = MeanAveragePrecision()\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader), colour='cyan', file=sys.stdout)\n",
        "    for step, (images, targets) in bar:\n",
        "        images = [img.to(device) for img in images]\n",
        "        images = torch.stack(images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        predictions = model(images, targets)\n",
        "\n",
        "        filtered_predictions = []\n",
        "        for pred in predictions:\n",
        "            boxes = pred['boxes']\n",
        "            scores = pred['scores']\n",
        "            labels = pred['labels']\n",
        "\n",
        "            # Filter boxes with low scores\n",
        "            keep = scores > score_threshold\n",
        "            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
        "\n",
        "            # Apply NMS\n",
        "            keep = nms(boxes, scores, iou_threshold)\n",
        "            filtered_predictions.append({\n",
        "                'boxes': boxes[keep],\n",
        "                'scores': scores[keep],\n",
        "                'labels': labels[keep]\n",
        "            })\n",
        "\n",
        "        # Update metric\n",
        "        metric.update(filtered_predictions, targets)\n",
        "\n",
        "    metrics = metric.compute()\n",
        "    print(f\"Mean Average Precision: {metrics['map'].item()}\")\n",
        "    print(f\"Mean Average Precision (50% IOU): {metrics['map_50'].item()}\")\n",
        "    print(f\"Mean Average Precision (75% IOU): {metrics['map_75'].item()}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def run_training(model, num_epochs, learning_rate):\n",
        "    # Define optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Check if we are using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    # For keeping track of the best validation mAP\n",
        "    top_map = 0.0\n",
        "\n",
        "    # We train the emodel for a number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, trainloader, device, optimizer, epoch)\n",
        "\n",
        "        # For validation we do not keep track of gradients\n",
        "        with torch.no_grad():\n",
        "            metrics = valid_epoch(model, testloader, device)\n",
        "            map_50 = metrics['map_50'].item()\n",
        "\n",
        "            if top_map < map_50:\n",
        "                print(f\"mAP-50 Improved ({top_map} ---> {map_50})\")\n",
        "                top_map = map_50\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pnW0GjmgM0k"
      },
      "outputs": [],
      "source": [
        "# You may have to play with the hyperparameters to obtain better results\n",
        "run_training(ssd_lite, 15, 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMVjTkTjgM0k"
      },
      "outputs": [],
      "source": [
        "# Visualize 5 images and bounding boxes from the train_loader\n",
        "images, targets = next(iter(trainloader))\n",
        "\n",
        "# Get predictions\n",
        "ssd_lite.eval()\n",
        "with torch.no_grad():\n",
        "    images = [img.to(device) for img in images]\n",
        "    predictions = ssd_lite(images)\n",
        "    images = [img.cpu() for img in images]\n",
        "\n",
        "visualize_images_and_boxes(images, targets, predictions, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlU6AiyqodDv"
      },
      "outputs": [],
      "source": [
        "# Visualize 5 images and bounding boxes from the test_loader\n",
        "images, targets = next(iter(testloader))\n",
        "\n",
        "# Get predictions\n",
        "ssd_lite.eval()\n",
        "with torch.no_grad():\n",
        "    images = [img.to(device) for img in images]\n",
        "    predictions = ssd_lite(images)\n",
        "    images = [img.cpu() for img in images]\n",
        "\n",
        "visualize_images_and_boxes(images, targets, predictions, num_samples=10, confidence_threshold=0.5, iou_threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSwkHZRf0hvv"
      },
      "source": [
        "Play with the ```confidence_threshold``` and ```iou_threshold``` values in the call to the ```visualize_images_and_boxes()``` function. What do you observe?\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
